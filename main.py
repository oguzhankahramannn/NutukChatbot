import os
import gradio as gr
from dotenv import load_dotenv

from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

print("Kurulum ve ayarlar yapÄ±lÄ±yor...")
load_dotenv()
if "GOOGLE_API_KEY" not in os.environ:
    raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. .env dosyanÄ±zÄ± kontrol edin.")

llm = ChatGoogleGenerativeAI(model="models/gemini-2.0-flash")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("Modeller yÃ¼klendi.")

print("Nutuk veri seti (.jsonl) yÃ¼kleniyor ve vektÃ¶r veritabanÄ± oluÅŸturuluyor... Bu iÅŸlem biraz zaman alabilir.")

loader = JSONLoader(
    file_path='nutuk_lora_dataset.jsonl',
    jq_schema='.content',
    json_lines=True,
    text_content=False)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
texts = text_splitter.split_documents(documents)

vector_store = Chroma.from_documents(texts, embeddings, collection_name="nutuk-lora-chat")
print("VeritabanÄ± hazÄ±r!")

# --- DEÄÄ°ÅÄ°KLÄ°K BURADA: Retriever'Ä± daha esnek ayarlarla yapÄ±landÄ±rÄ±yoruz ---
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={'score_threshold': 0.5, 'k': 3}
)
# AÃ§Ä±klama:
# search_type="similarity_score_threshold": Arama tipini "benzerlik skoru eÅŸiÄŸi" olarak ayarlar.
# score_threshold: 0.5: Benzerlik skoru 0.5'in Ã¼zerindeki sonuÃ§larÄ± 'ilgili' olarak kabul etmesini saÄŸlar. (Bu eÅŸiÄŸi dÃ¼ÅŸÃ¼rdÃ¼k).
# k: 3: En fazla 3 adet ilgili dokÃ¼man getirmesini sÃ¶yler.
# --------------------------------------------------------------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever  # Yeni ve daha akÄ±llÄ± retriever'Ä±mÄ±zÄ± kullanÄ±yoruz
)


def get_rag_response(question):
    print(f"Sorgu alÄ±ndÄ±: {question}")
    result = qa_chain.invoke({"query": question})
    return result['result']

iface = gr.Interface(
    fn=get_rag_response,
    inputs=gr.Textbox(lines=5, placeholder="Nutuk ile ilgili bir soru sorun..."),
    outputs="text",
    title="ğŸ“œ Nutuk DanÄ±ÅŸmanÄ± Chatbot",
    description="Bu chatbot, Nutuk metinleri ile eÄŸitilmiÅŸtir. SorularÄ±nÄ±za bu kaynaktan cevaplar Ã¼retecektir."
)

iface.launch()